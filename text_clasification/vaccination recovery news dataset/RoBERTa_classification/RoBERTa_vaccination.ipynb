{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of target value:\n",
      " target\n",
      "1    0.652361\n",
      "0    0.347639\n",
      "Name: proportion, dtype: float64\n",
      "TRAIN Dataset shape: (559,)\n",
      "TEST Dataset shape: (140,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Training Loss: 0.6764, Training Accuracy: 62.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 50/70 [06:17<02:33,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Training Loss: 0.6183, Training Accuracy: 66.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 70/70 [08:50<00:00,  7.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.5913, Accuracy: 66.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Training Loss: 0.2701, Training Accuracy: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 50/70 [07:28<03:41, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Training Loss: 0.3496, Training Accuracy: 88.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 70/70 [11:21<00:00,  9.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.3400, Accuracy: 88.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Training Loss: 0.0974, Training Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 50/70 [09:37<03:59, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Training Loss: 0.2341, Training Accuracy: 92.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 70/70 [13:26<00:00, 11.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.2265, Accuracy: 92.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Training Loss: 0.0723, Training Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 50/70 [09:40<03:53, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Training Loss: 0.1249, Training Accuracy: 96.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 70/70 [13:35<00:00, 11.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.1566, Accuracy: 95.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:57<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1527, Accuracy: 94.29%\n",
      "Precision: 0.96, Recall: 0.96\n",
      "Test Accuracy: 94.29%\n",
      "Test Precision: 95.60%\n",
      "Test Recall: 95.60%\n",
      "Model state_dict saved to c:\\Users\\Lukag\\OneDrive\\Documents\\git\\IJS\\text_clasification_2\\news_classifier_model\\RoBERTa_classification\\saved_model\\pytorch_roberta_vaccination.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vocabulary path (c:\\Users\\Lukag\\OneDrive\\Documents\\git\\IJS\\text_clasification_2\\news_classifier_model\\RoBERTa_classification\\saved_model\\pytorch_roberta_vaccination.bin) should be a directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary saved to c:\\Users\\Lukag\\OneDrive\\Documents\\git\\IJS\\text_clasification_2\\news_classifier_model\\RoBERTa_classification\n",
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "two_steps_back = os.path.dirname(os.path.dirname(current_dir))\n",
    "sys.path.append(two_steps_back)\n",
    "from RoBERTa_classifier import RobertaTrainer, RobertaTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CONSTANTS & VARIABLES\n",
    "# =============================================================================\n",
    "# Device configuration\n",
    "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data paths\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "TWO_FOLDERS_BACK = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "DATA_FOLDER = os.path.join(TWO_FOLDERS_BACK, 'data')\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, 'MMCoVaR_News_Dataset.csv')\n",
    "\n",
    "# Hyperparameters and settings\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-05\n",
    "PADDING = \"max_length\"\n",
    "RANDOM_STATE = 200\n",
    "\n",
    "# Columns for text and target\n",
    "TEXT_COLUMN = \"clean_text\"  # Make sure your DataFrame has this column; otherwise adjust accordingly.\n",
    "TARGET_COLUMN = \"target\"\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATA UPLOAD & PREPARATION\n",
    "# =============================================================================\n",
    "# ____________________________________________________________\n",
    "# Load the  News Dataset\n",
    "\n",
    "two_steps_back = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_path = os.path.join(two_steps_back, 'data')\n",
    "vaccination_recovery_news = pd.read_csv(os.path.join(data_path, 'vaccination-recovery-news-data.csv'))\n",
    "vaccination_recovery_news['synthetic'] = False\n",
    "\n",
    "# Create a DataFrame for original articles with label 1\n",
    "vaccination_recovery_news_train = vaccination_recovery_news[['body_text', 'reliability']].copy()\n",
    "vaccination_recovery_news_train.columns = [TEXT_COLUMN, TARGET_COLUMN]\n",
    "vaccination_recovery_news_train['synthetic'] = False\n",
    "\n",
    "df = vaccination_recovery_news_train\n",
    "\n",
    "# Print the ratio between the classes\n",
    "\n",
    "print(\"Ratio of target value:\\n\",df['target'].value_counts(normalize = True))\n",
    "\n",
    "# =============================================================================\n",
    "# 3. RUN ROBERTA\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Roberta tokenizer.\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', truncation=True, do_lower_case=True)\n",
    "\n",
    "\n",
    "# Create an instance of our trainer.\n",
    "trainer = RobertaTrainer(\n",
    "    df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    valid_batch_size=VALID_BATCH_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    device=DEVICE,\n",
    "    padding=PADDING,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    random_state=RANDOM_STATE,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "# Step 2: Tokenize the data and create DataLoaders.\n",
    "trainer.tokenize_data()\n",
    "\n",
    "# Step 3: Build the model.\n",
    "trainer.build_model()\n",
    "\n",
    "# Step 4: Train the model.\n",
    "trainer.train_model(epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Step 5: Evaluate the model.\n",
    "acc, prec, rec = trainer.evaluate_model()\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(f\"Test Precision: {prec * 100:.2f}%\")\n",
    "print(f\"Test Recall: {rec * 100:.2f}%\")\n",
    "\n",
    "# Save the model and tokenizer.\n",
    "output_model_file = 'pytorch_roberta_vaccination.bin'\n",
    "output_tokenizer_file = 'pytorch_roberta_vaccination_tokenizer.bin'\n",
    "output_vocab_file = 'results_ROBERTA'\n",
    "directory_to_save = os.path.join(os.getcwd(), \"saved_model\", output_model_file)\n",
    "directory_to_save_tokenizer = os.path.join(os.getcwd(), \"saved_model\", output_tokenizer_file)\n",
    "\n",
    "trainer.save_model(os.path.join(os.getcwd(), \"saved_model\", output_model_file), os.getcwd())\n",
    "tokenizer.save_vocabulary(directory_to_save, directory_to_save_tokenizer)\n",
    "print('Model and tokenizer saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
